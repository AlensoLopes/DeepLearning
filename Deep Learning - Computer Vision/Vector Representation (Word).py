# -*- coding: utf-8 -*-
"""Vector Representation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oDjuuKrt0NgmSMWm4PsFdYoVQCY3h9Op
"""

import io
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url, untar=True, cache_dir='.', cache_subdir='')
dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
os.listdir(dataset_dir)

train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)

remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)

batch_size = 1024
seed = 1

train_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, seed=seed, validation_split=0.2, subset='training')
val_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, seed=seed, validation_split=0.2, subset='validation')

for text_batch, label_batch in train_ds.take(1):
  for i in range(5):
    print(label_batch[i].numpy(), text_batch.numpy()[i])

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

embedding_layer = tf.keras.layers.Embedding(1000,5)

result = embedding_layer(tf.constant([1,2,3]))
result.numpy()

result = embedding_layer(tf.constant([[0,1,2], [3,4,5]]))
result.shape

def standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')

vocab_size = 1000
sequence_length = 100

vectorize_layer = TextVectorization(standardize=standardization, max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)

text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)

embedding_dim = 16

model = Sequential([
                    vectorize_layer,
                    Embedding(vocab_size, embedding_dim, name='embedding'),
                    GlobalAveragePooling1D(),
                    Dense(16, activation='relu'),
                    Dense(1)
])

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')

model.compile(optimizer='adam', loss=tf.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])

model.fit(train_ds, validation_data=val_ds, epochs=1, callbacks=[tensorboard_callback])

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

weights = model.get_layer('embedding').get_weights()[0]
vocab = vectorize_layer.get_vocabulary()

out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')

for index, word in enumerate(vocab):
  if index == 0:
    continue
  vec = weights[index]
  out_v.write('\t'.join([str(x) for x in vec]) + '\n')
  out_m.write(word + '\n')
out_v.close()
out_m.close()

try:
  from google.colab import files
    files.download('vector.tsv')
    files.download('metadata.tsv') 
  except Exception:
    pass

